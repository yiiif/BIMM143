---
title: "Class 9"
subtitle: "Unsupervised learning mini-project"
author: "Yi Fu"
date: "4/30/2019"
output: html_document
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

First, let's check if "factoextra" and "rgl" package are installed. And then, load the packages.
```{r warning=FALSE, include=FALSE, results='hide'}
if (!require("factoextra")) {
  install.packages("factoextra")
}
if (!require("rgl")) {
  install.packages("rgl")
}
```

```{r}
library("factoextra")
library("rgl")
knitr::knit_hooks$set(webgl = hook_webgl)
```

## Preparing the data for Human Breast Cancer Cells

The **WisconsinCancer.csv** containts values describing characteristics of the cell nuclei present in digitized images of a fine needle aspiration (FNA) of a breast mass. There are 569 samples. Among these, there are 212 malignant samples and 357 benign samples.
```{r}
df = read.csv("data/WisconsinCancer.csv")
data = as.matrix(df[,3:32])
rownames(data) = df$id
diagnosis = as.numeric(df$diagnosis=="M")
```

```{r include=FALSE}
sum(df$diagnosis=="M")
sum(df$diagnosis=="B")
```

**It is important to check if the data need to be scaled before performing PCA. **

The following suggests that the data should be scaled.
```{r}
as.vector(round(apply(data,2,mean)))
as.vector(round(apply(data,2,sd)))
```

So, let's scale the data
```{r}
data.scaled = scale(data)
```

## Performing PCA
Let's use the *prcomp()* function.
```{r}
pca = prcomp(data.scaled)
summary(pca)
```

> From your results, 44.27% of the original variance is captured by the first principal components.

> 7 principal components are required to describe at least 90% of the original variance in the data.

Here is the plot for the percentage of variance captured by each PC:
```{r}
pca.var = pca$sdev^2
pca.var.per = round(pca.var/sum(pca.var)*100, 1)
barplot(pca.var.per, 
        ylab="Precent of Variance Explained", names.arg=paste0("PC",1:length(pca.var.per)),
        las=2, axes = FALSE)
axis(2, at=pca.var.per, labels=round(pca.var.per,2))
```

Here is the side-by-side plot for the percentage and cumulative percentage of variance captured by each PC:
```{r}
par(mfcol=c(1,2))
plot(pca.var.per,
     xlab="Principal Component", ylab="Percentage of Variance Explained (%)", ylim=c(0, 100), 
     type="o")
plot(cumsum(pca.var.per), 
     xlab="Principal Component", ylab="Cumulative Percentage of Variance Explained (%)", ylim=c(0, 100), 
     type="o")
```

Here is the plot generated by "factoextra" package:
```{r}
fviz_eig(pca, addlabels = TRUE)
```

Here is the biplot:
```{r}
biplot (pca)
```

Here is the plot:
```{r}
plot (pca$x[,1], pca$x[,2],
      xlab=paste0("PC1 (", pca.var.per[1], "%)"), ylab=paste0("PC2 (", pca.var.per[2], "%)"),
      col=diagnosis + 1, pch=16)
```

Here is the plot (PC1 vs PC3):
```{r}
plot (pca$x[,1], pca$x[,3],
      xlab=paste0("PC1 (", pca.var.per[1], "%)"), ylab=paste0("PC3 (", pca.var.per[3], "%)"),
      col=diagnosis + 1, pch=16)
```

Here is the 3D plot (PC1 vs PC2 vs PC3):
```{r webgl=TRUE}
plot3d(pca$x[,1], pca$x[,2], pca$x[,3],
       xlab=paste0("PC1 (", pca.var.per[1], "%)"), ylab=paste0("PC2 (", pca.var.per[2], "%)"), 
       zlab=paste0("PC3 (", pca.var.per[3], "%)"), 
       col=diagnosis + 1, cex=1.5, size=1, type="s")

# this is needed to generate the plot in regular .r file.
#rglwidget(width = 400, height = 400)
```

```{r eval=FALSE, include=FALSE}
# For the first principal component, what is the component of the loading vector for the feature concave.points_mean?
pca$rotation["concave.points_mean",1]
```

## Performing Hierarchical Clustering

We need a distance matrix not the raw data as input.
```{r}
dist = dist(data.scaled)
```

Let's use the *hclust()* function.
```{r}
hc = hclust(dist, method="complete")
```

Here is the plot:
```{r}
plot(hc)
abline(h=15, col="red", lty=2)
```

Let's compare the known clusters with hierarchically generated clusters. The following is the best cluster vs diagnoses match with by cutting into a different number of clusters between 2 and 10:
```{r}
gp5 = cutree(hc, 5)
table(gp5, diagnosis)
```

## Performing K-mean Clustering

Let's use the *kmeans()* function.
```{r}
two_means = kmeans(data.scaled, centers=2, nstart=20)
```

Let's compare the known clusters with 2-mean clusters.
```{r}
table(two_means$cluster, diagnosis)
```

Let's compare hierarchically generated clusters with 2-mean clusters.
```{r}
gp2 = cutree(hc, 2)
table(gp2, two_means$cluster)

gp4 = cutree(hc, 4)
table(gp4, two_means$cluster)
```

## Combining PCA and Hierarchical Clustering
Using the minimum number of principal components required to describe at least 90% of the variability in the data, create a hierarchical clustering model with the linkage method="ward.D2". We use Ward’s criterion here because it is based on multidimensional variance like principal components analysis. Assign the results to hc.ward.
```{r}
hc.ward = hclust(dist(pca$x[,1:7]), method="ward.D2")
plot(hc.ward)
```

Let's compare the known clusters with hierarchically generated clusters (ward.D2).
```{r}
gp2.ward = cutree(hc.ward, k=2)
table(gp2.ward, diagnosis)
```

Let's compare hierarchically generated clusters (ward.D2) with PCA.
```{r}
plot(pca$x[,1:2], col = -gp2.ward + 3, pch=16)
plot(pca$x[,1:2], col = diagnosis + 1, pch=16)
```

## Sensitivity vs Specificity
Sensitivity refers to a test’s ability to correctly detect ill patients who do have the condition. In our example here the sensitivity is the total number of samples in the cluster identified as predominantly malignant (cancerous) divided by the total number of known malignant samples.

Specificity relates to a test’s ability to correctly reject healthy patients without a condition. In our example specificity is the proportion of benign (not cancerous) samples in the cluster identified as predominantly benign that are known to be benign.

> Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?

diagnosis: 0 is benign, 1 is malignant.
gp2.ward: 1 is malignant, 2 is benign.
gp2: 1 is benign, 2 is malignant.
two_means$cluster: 1 is malignant, 2 is benign.
```{r}
table(gp2.ward, diagnosis) # PCA(1:7 component): most sensitive
table(gp2, diagnosis) # HClust (2 cluster): most specific
table(two_means$cluster, diagnosis) #K-mean (2 cluster)
```


## Prediction
The **new_samples.csv** contains the patient information for us to predict whether their cancer are likely to be benign or malignant.
```{r}
newdata = read.csv("data/new_samples.csv")
```

Let's use the *predict()* function. This makes the projection of new cancer cell data onto our PCA model.
```{r}
pca = prcomp(data,scale=T)
npc = predict(pca, newdata=newdata)
npc
```

```{r}
plot(pca$x[,1:2], col=gp2.ward)
points(npc[,1], npc[,2], col = "blue", pch = 16, cex = 3)
text (npc[,1], npc[,2], c(1,2), col = "white")
```

> The first patient in malignant cluster, and we should prioritize the first patient for follow up based on your results.

